{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO_RLHF.ppo_rlhf import PPORLHF\n",
    "from PPO_RLHF.ppo import PPO\n",
    "from PPO_RLHF.networks import ActorNetwork\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preferred dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for policies\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "\n",
    "# TO COMPLETE, choose the file names of the desired policies\n",
    "\n",
    "pi1_path = os.path.join(PATH, 'saved_policies', 'pi1_cartpole_1.pt')\n",
    "\n",
    "pi2_path = os.path.join(PATH, 'saved_policies', 'pi2_cartpole_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_preference_dataset import dataset\n",
    "\n",
    "n_trajectories = 1000\n",
    "max_t=500\n",
    "seed = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "dim_state = 4\n",
    "\n",
    "trajectories_states_pi1, trajectories_actions_pi1, trajectories_states_pi2, trajectories_actions_pi2, trajectories_rewards_pi1, trajectories_rewards_pi2 = dataset(\n",
    "    n_trajectories, max_t, seed, pi1_path, pi2_path, env, dim_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate K pairs of trajectories and preferences\n",
    "K = 100  # Number of preference pairs to generate\n",
    "preferences = []\n",
    "trajectory_pairs = []\n",
    "\n",
    "for k in range(K):\n",
    "    # Randomly select two different trajectory indices\n",
    "    idx1 = np.random.randint(0, len(trajectories_rewards_pi1))\n",
    "    idx2 = np.random.randint(0, len(trajectories_rewards_pi2))\n",
    "    \n",
    "    # Get the states and actions for both trajectories\n",
    "    traj1_states = trajectories_states_pi1[idx1]\n",
    "    traj1_actions = trajectories_actions_pi1[idx1]\n",
    "    traj2_states = trajectories_states_pi2[idx2] \n",
    "    traj2_actions = trajectories_actions_pi2[idx2]\n",
    "    \n",
    "    # Store the trajectory pair\n",
    "    trajectory_pairs.append([\n",
    "        (traj1_states, traj1_actions),\n",
    "        (traj2_states, traj2_actions)\n",
    "    ])\n",
    "    \n",
    "    # Calculate preference probability using the softmax formula\n",
    "    reward1 = trajectories_rewards_pi1[idx1]\n",
    "    reward2 = trajectories_rewards_pi2[idx2]\n",
    "    \n",
    "    # P(τ1) = exp(R(τ1)) / (exp(R(τ1)) + exp(R(τ2)))\n",
    "    prob_traj1 = np.exp(reward1) / (np.exp(reward1) + np.exp(reward2))\n",
    "    preferences.append(prob_traj1)\n",
    "\n",
    "preferences = np.array(preferences)\n",
    "print(f\"Generated {K} preference pairs\")\n",
    "print(f\"Average preference probability for first trajectory: {np.mean(preferences):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the preference dataset for the reward model\n",
    "formatted_preference_dataset = []\n",
    "\n",
    "for i in range(len(trajectory_pairs)):\n",
    "    traj_pair = trajectory_pairs[i]\n",
    "    preference = preferences[i]\n",
    "    formatted_preference_dataset.append([\n",
    "        traj_pair[0],  # (states1, actions1) for trajectory 1\n",
    "        traj_pair[1],  # (states2, actions2) for trajectory 2\n",
    "        preference     # preference probability for trajectory 1\n",
    "    ])\n",
    "\n",
    "print(\"\\nFormatted preference dataset:\")\n",
    "print(f\"Number of trajectory pairs: {len(formatted_preference_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO-RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and magic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'timesteps_per_batch': 4080,\n",
    "    'max_timesteps_per_episode': 1600,\n",
    "    'gamma': 0.95,\n",
    "    'n_updates_per_iteration': 5,\n",
    "    'lr': 0.005,\n",
    "    'clip': 0.2,\n",
    "    'render': True,\n",
    "    'render_every_i': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [42, 123, 456]\n",
    "TOTAL_TIMESTEPS = 50000\n",
    "ENV_NAME = ['CartPole-v1', 'MountainCar-v0']\n",
    "EVAL_N_EPISODES = 5\n",
    "EVAL_LEARNING_RATE = 0.005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the PPO-RLHF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode = []\n",
    "rewards_per_seed = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    # Create the environment\n",
    "    env = gym.make(ENV_NAME[0])\n",
    "    model = PPORLHF(\n",
    "        env=env,\n",
    "        preference_data=formatted_preference_dataset,\n",
    "        seed=seed,\n",
    "        **hyperparameters\n",
    "    )\n",
    "    model.learn(TOTAL_TIMESTEPS)\n",
    "\n",
    "    env.close()\n",
    "    env = gym.make(ENV_NAME[0], render_mode=\"human\")\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n\n",
    "    else:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    actor = ActorNetwork(action_dim, obs_dim, alpha=EVAL_LEARNING_RATE)\n",
    "    actor.load_state_dict(torch.load(os.path.join(\"PPO_RLHF\", \"models\", 'ppo-rlhf_actor.pth')))\n",
    "    actor.eval()\n",
    "\n",
    "    device = actor.device\n",
    "\n",
    "    # ---- Run evaluation ----\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(EVAL_N_EPISODES):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, get action probabilities and sample\n",
    "                    action_probs = actor(obs_tensor)\n",
    "                    dist = Categorical(action_probs)\n",
    "                    action = dist.sample()\n",
    "                    action = action.item()  # Convert to Python scalar\n",
    "                else:\n",
    "                    # For continuous actions, get the action directly and clip it\n",
    "                    action = actor(obs_tensor)\n",
    "                    action = action.cpu().numpy()\n",
    "                    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            time.sleep(0.02)\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    rewards_per_seed.append(rewards_per_episode)\n",
    "\n",
    "    # ---- Plot reward results ----\n",
    "    plt.plot(total_reward, marker='o')\n",
    "    plt.title(\"PPO-RLHF - Evaluation: Seed Rewards\")\n",
    "    plt.xlabel(\"Seeds\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode = []\n",
    "rewards_per_seed = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    # Create the environment\n",
    "    env = gym.make(ENV_NAME[0])\n",
    "    model = PPO(\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "        **hyperparameters\n",
    "    )\n",
    "    model.learn(TOTAL_TIMESTEPS)\n",
    "\n",
    "    env.close()\n",
    "    env = gym.make(ENV_NAME[0], render_mode=\"human\")\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n\n",
    "    else:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "    actor = ActorNetwork(action_dim, obs_dim, alpha=EVAL_LEARNING_RATE)\n",
    "    actor.load_state_dict(torch.load(os.path.join(\"PPO_RLHF\", \"models\", 'ppo_actor.pth')))\n",
    "    actor.eval()\n",
    "\n",
    "    device = actor.device\n",
    "\n",
    "    # ---- Run evaluation ----\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(EVAL_N_EPISODES):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, get action probabilities and sample\n",
    "                    action_probs = actor(obs_tensor)\n",
    "                    dist = Categorical(action_probs)\n",
    "                    action = dist.sample()\n",
    "                    action = action.item()  # Convert to Python scalar\n",
    "                else:\n",
    "                    # For continuous actions, get the action directly and clip it\n",
    "                    action = actor(obs_tensor)\n",
    "                    action = action.cpu().numpy()\n",
    "                    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            time.sleep(0.02)\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
    "\n",
    "    rewards_per_seed.append(rewards_per_episode)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # ---- Plot reward results ----\n",
    "    #plt.plot(total_reward, marker='o')\n",
    "    #plt.title(\"PPO - Evaluation: Seed Rewards\")\n",
    "    #plt.xlabel(\"Seeds\")\n",
    "    #plt.ylabel(\"Total Reward\")\n",
    "    #plt.grid(True)\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
