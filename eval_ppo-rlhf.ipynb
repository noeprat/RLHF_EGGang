{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO_RLHF.ppo_rlhf import PPORLHF\n",
    "from PPO_RLHF.ppo import PPO\n",
    "from PPO_RLHF.networks import ActorNetwork\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    'timesteps_per_batch': 2048,      # Reduced batch size for faster learning\n",
    "    'max_timesteps_per_episode': 500,  # CartPole episodes typically end within 500 steps\n",
    "    'gamma': 0.99,                     # Higher discount factor for CartPole\n",
    "    'n_updates_per_iteration': 10,     # More updates per iteration\n",
    "    'lr': 0.0003,                     # Lower learning rate for stability\n",
    "    'clip': 0.2,                      # Standard PPO clipping\n",
    "    'render': False,\n",
    "    'render_every_i': 10,\n",
    "    'reward_model_epochs': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [42, 43, 44]\n",
    "TOTAL_TIMESTEPS = 100000\n",
    "ENV_NAME = ['CartPole-v1', 'MountainCar-v0']\n",
    "EVAL_N_EPISODES = 10\n",
    "EVAL_LEARNING_RATE = 0.005\n",
    "PREFERENCE_DATA_SIZE = [10, 50, 100, 300, 500, 700, 1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_dataset = []\n",
    "raw_data = dataset\n",
    "# Format the preference dataset correctly\n",
    "preference_dataset = []\n",
    "for i in range(len(raw_data[0])):  # Iterate over trajectories\n",
    "    traj1_states = raw_data[0][i]  # States from policy 1\n",
    "    traj1_actions = raw_data[1][i]  # Actions from policy 1\n",
    "    traj2_states = raw_data[2][i]  # States from policy 2\n",
    "    traj2_actions = raw_data[3][i]  # Actions from policy 2\n",
    "    preference = raw_data[4][i]     # Preference probability\n",
    "    \n",
    "    # Format each trajectory pair\n",
    "    traj_pair = [\n",
    "        (traj1_states, traj1_actions),  # First trajectory\n",
    "        (traj2_states, traj2_actions),  # Second trajectory\n",
    "        preference                      # Preference probability\n",
    "    ]\n",
    "    preference_dataset.append(traj_pair)\n",
    "\n",
    "# Create different sized datasets\n",
    "formatted_datasets = []\n",
    "for size in PREFERENCE_DATA_SIZE:\n",
    "    data = preference_dataset[:size]\n",
    "    formatted_datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(formatted_preference_dataset, seed):\n",
    "    print(\"Evaluating the model...\")\n",
    "    env = gym.make(ENV_NAME[0])\n",
    "    rewards_per_experiment = []\n",
    "\n",
    "    model = PPORLHF(\n",
    "        env=env,\n",
    "        preference_data=formatted_preference_dataset,\n",
    "        seed=seed,\n",
    "        **cartpole_hyperparameters\n",
    "    )\n",
    "    model.learn(TOTAL_TIMESTEPS)\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n\n",
    "    else:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    actor = ActorNetwork(action_dim, obs_dim, alpha=EVAL_LEARNING_RATE)\n",
    "    actor.load_state_dict(torch.load(os.path.join(\"PPO_RLHF\", \"models\", 'ppo-rlhf_actor.pth')))\n",
    "    actor.eval()\n",
    "\n",
    "    device = actor.device\n",
    "\n",
    "    # ---- Run evaluation ----\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for ep in range(EVAL_N_EPISODES):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, get action probabilities and sample\n",
    "                    action_probs = actor(obs_tensor)\n",
    "                    dist = Categorical(action_probs)\n",
    "                    action = dist.sample()\n",
    "                    action = action.item()  # Convert to Python scalar\n",
    "                else:\n",
    "                    # For continuous actions, get the action directly and clip it\n",
    "                    action = actor(obs_tensor)\n",
    "                    action = action.cpu().numpy()\n",
    "                    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            #time.sleep(0.02)\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    rewards_per_experiment.append(rewards_per_episode)\n",
    "\n",
    "    return rewards_per_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_rew = []\n",
    "\n",
    "for i in range(len(PREFERENCE_DATA_SIZE)):\n",
    "    seed_rew = []\n",
    "    print(f\"===== Running for Preference Data size {PREFERENCE_DATA_SIZE[i]}... =====\")\n",
    "    # Run for each seed\n",
    "    for seed in SEEDS:\n",
    "        print(f\"---- Running for seed number {seed}... ----\")\n",
    "        reward = run_experiment(formatted_datasets[i], seed)\n",
    "        seed_rew.append(reward)\n",
    "    \n",
    "    # Calculate mean and std across seeds\n",
    "    preference_rew.append(seed_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate mean and confidence intervals for each preference data size\n",
    "means = []\n",
    "lower_ci = []\n",
    "upper_ci = []\n",
    "confidence_level = 0.95  # 95% confidence interval\n",
    "\n",
    "for size_rewards in preference_rew:\n",
    "    # Flatten rewards across seeds and episodes\n",
    "    all_rewards = [reward for seed_rewards in size_rewards for reward in seed_rewards[0]]\n",
    "    \n",
    "    mean = np.mean(all_rewards)\n",
    "    std = np.std(all_rewards)\n",
    "    n = len(all_rewards)\n",
    "    \n",
    "    # Calculate confidence interval using t-distribution\n",
    "    t_value = stats.t.ppf((1 + confidence_level) / 2, n-1)\n",
    "    margin_of_error = t_value * (std / np.sqrt(n))\n",
    "    \n",
    "    means.append(mean)\n",
    "    lower_ci.append(mean - margin_of_error)\n",
    "    upper_ci.append(mean + margin_of_error)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot mean line\n",
    "plt.plot(PREFERENCE_DATA_SIZE, means, label='Mean', marker='o')\n",
    "\n",
    "plt.xlabel('Preference Dataset Size')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title(f'PPO-RLHF Performance for environment {ENV_NAME[0]}')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f'ppo-rlhf_performance_{ENV_NAME[0]}_Final2.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
