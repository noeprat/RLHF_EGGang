{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f0307d",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b09b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the imports -- no need to change this\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15548740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abae4aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe27b13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01369617, -0.02302133, -0.04590265, -0.04834723], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "seed = 0\n",
    "env.reset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b87446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classes import Policy\n",
    "\n",
    "import utils.classes\n",
    "torch.serialization.add_safe_globals([getattr, torch.nn.modules.linear.Linear, utils.classes.Policy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcdb84",
   "metadata": {},
   "source": [
    "# Testing baselines for REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5569652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.baselines import naive_baseline, baseline_1, baseline_2\n",
    "\n",
    "from utils.reinforce import reinforce_rwd2go, reinforce_rwd2go_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e32c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = bool\n",
    "run=False\n",
    "\n",
    "if run:\n",
    "    # naive baseline\n",
    "    policy_naive_baseline = Policy().to(device)\n",
    "    optimizer_naive_baseline = optim.Adam(policy_naive_baseline.parameters(), lr=1e-2)\n",
    "    scores_naive_baseline = reinforce_rwd2go_baseline(policy_naive_baseline, optimizer_naive_baseline, seed=seed, baseline=naive_baseline, env=env, early_stop=True, n_episodes=200)\n",
    "\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "    # baseline 1\n",
    "    policy_baseline_1 = Policy().to(device)\n",
    "    optimizer_baseline_1 = optim.Adam(policy_baseline_1.parameters(), lr=1e-2)\n",
    "    scores_baseline_1 = reinforce_rwd2go_baseline(policy_baseline_1, optimizer_baseline_1, seed=seed, baseline=baseline_1, env=env, early_stop=True, n_episodes=200)\n",
    "\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    # baseline 2\n",
    "    policy_baseline_2 = Policy().to(device)\n",
    "    optimizer_baseline_2 = optim.Adam(policy_baseline_2.parameters(), lr=1e-2)\n",
    "    scores_baseline_2 = reinforce_rwd2go_baseline(policy_baseline_2, optimizer_baseline_2, seed=seed, baseline=baseline_2, env=env, early_stop=True, n_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4d9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:    \n",
    "### Plot the learning progress\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot the scores with specified colors and labels\n",
    "    #ax.plot(np.arange(1, len(scores_rwd2go) + 1), scores_rwd2go, color='green', label='No Baseline')\n",
    "    ax.plot(np.arange(1, len(scores_baseline_1) + 1), scores_baseline_1, color='blue', label='Baseline 1')\n",
    "    ax.plot(np.arange(1, len(scores_baseline_2) + 1), scores_baseline_2, color='red', label='Baseline 2')\n",
    "    ax.plot(np.arange(1, len(scores_naive_baseline) + 1), scores_naive_baseline, color='black', label='Naive Baseline')\n",
    "\n",
    "    # Set the labels with a larger font size\n",
    "    ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "    ax.set_xlabel('Episode #', fontsize=20)\n",
    "\n",
    "    # Set the tick labels to a larger font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add a legend with a specified font size\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0b5ce",
   "metadata": {},
   "source": [
    "# Generating the preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8702e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_preference_dataset import generate_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "047cbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy saving\n",
    "run=False\n",
    "if run:\n",
    "\n",
    "    np.bool8 = bool\n",
    "    seed = 0\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env.reset(seed=seed)\n",
    "    target_score = 500\n",
    "    n_episodes = 500\n",
    "    print_every = 500\n",
    "    max_t = 1000\n",
    "    baseline = naive_baseline\n",
    "\n",
    "    example_policy = Policy().to(device)\n",
    "    optimizer_baseline_1 = optim.Adam(example_policy.parameters(), lr=1e-2)\n",
    "    example_scores = reinforce_rwd2go_baseline(example_policy, optimizer_baseline_1, seed=seed, env=env, target_score=target_score, baseline=baseline, early_stop=False, n_episodes=n_episodes, max_t=max_t, print_every=print_every, save_models_every=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e0846",
   "metadata": {},
   "source": [
    "## Loading policies (manual intervention required)\n",
    "\n",
    "Rename the policies you want to choose as $\\pi_1$ and $\\pi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f43a8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.getcwd())\n",
    "\n",
    "# TO COMPLETE, choose the file names of the desired policies\n",
    "\n",
    "pi1_path = os.path.join(PATH, 'saved_policies', 'pi1_cartpole_0.pt')\n",
    "\n",
    "pi2_path = os.path.join(PATH, 'saved_policies', 'pi2_cartpole_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ecf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on K trajectories\n",
    "run=False\n",
    "if run:\n",
    "\n",
    "    seeds = [0,1,2]\n",
    "    n_trajectories = 500\n",
    "    max_t=1000\n",
    "    rewards_for_all_seeds_pi1 = np.zeros((n_trajectories, len(seeds)))\n",
    "    rewards_for_all_seeds_pi2 = np.zeros((n_trajectories, len(seeds)))\n",
    "\n",
    "    for seed_index, seed in enumerate(seeds):\n",
    "        pi1 = Policy().to(device)\n",
    "        pi1.load_state_dict(torch.load(pi1_path, weights_only=True))\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        trajectories_rewards_pi1, trajectories_states_pi1, trajectories_actions_pi1 = generate_trajectories(pi1, n_trajectories, env=env, max_t=max_t, seed = seed, dim_state=4)\n",
    "\n",
    "        rewards_for_all_seeds_pi1[:,seed_index] = trajectories_rewards_pi1[:]\n",
    "\n",
    "        pi2 = Policy().to(device)\n",
    "        pi2.load_state_dict(torch.load(pi2_path, weights_only=True))\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        trajectories_rewards_pi2, trajectories_states_pi2, trajectories_actions_pi2 = generate_trajectories(pi2, n_trajectories, env=env, max_t=max_t, seed = seed, dim_state=4)\n",
    "\n",
    "        rewards_for_all_seeds_pi2[:,seed_index] = trajectories_rewards_pi2[:]\n",
    "\n",
    "    rewards_avg_seeds_pi1 = np.mean(rewards_for_all_seeds_pi1, axis=1)\n",
    "    rewards_avg_seeds_pi2 = np.mean(rewards_for_all_seeds_pi2, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca27d2",
   "metadata": {},
   "source": [
    "## Making sure $\\pi_1$ performs $\\sim$ twice as good as $\\pi_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c70753",
   "metadata": {},
   "outputs": [],
   "source": [
    "run=False\n",
    "if run:\n",
    "    avg_pi1 = np.mean(rewards_avg_seeds_pi1)\n",
    "    avg_pi2 = np.mean(rewards_avg_seeds_pi2)\n",
    "    print(avg_pi1, avg_pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5478b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run=False\n",
    "if run:\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot the scores with specified colors and labels\n",
    "    #ax.plot(np.arange(1, len(scores_rwd2go) + 1), scores_rwd2go, color='green', label='No Baseline')\n",
    "    ax.plot(np.arange(1, n_trajectories + 1), rewards_avg_seeds_pi1, color='blue', label='Trajectories generated from pi_1')\n",
    "    ax.plot(np.arange(1, n_trajectories + 1), rewards_avg_seeds_pi2, color='red', label='Trajectories generated from pi_2')\n",
    "\n",
    "    # Set the labels with a larger font size\n",
    "    ax.set_ylabel('Total reward, \\n averaged on 3 seeds', fontsize=20)\n",
    "    ax.set_xlabel('Trajectory #', fontsize=20)\n",
    "\n",
    "    # Set the tick labels to a larger font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add a legend with a specified font size\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804adcb9",
   "metadata": {},
   "source": [
    "## Preference dataset\n",
    "\n",
    "The goal is to get a set $\\left\\{(s_i, a_i^+, a_i^-) \\in \\mathcal{S}\\times \\mathcal{A}\\times \\mathcal{A} \\; | \\; 0\\leq i \\leq K   \\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab26f0",
   "metadata": {},
   "source": [
    "### State sampling method 1:\n",
    "\n",
    "Run trajectories with $\\pi_1$ and $\\pi_2$, and complete with the rejected or preferred action remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a583a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First half \n",
      "\n",
      "\n",
      " Second half \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.generate_preference_dataset import generate_preference_dataset\n",
    "dataset_size = 1000\n",
    "seed=0\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "print_every = 10\n",
    "max_t = 999\n",
    "sampling_method='pi1_pi2_trajectories'\n",
    "\n",
    "pi1_path = os.path.join(PATH, 'saved_policies', 'pi1_cartpole_0.pt')\n",
    "pi1 = Policy().to(device)\n",
    "pi1.load_state_dict(torch.load(pi1_path, weights_only=True))\n",
    "\n",
    "pi2_path = os.path.join(PATH, 'saved_policies', 'pi2_cartpole_0.pt')\n",
    "pi2 = Policy().to(device)\n",
    "pi2.load_state_dict(torch.load(pi2_path, weights_only=True))\n",
    "\n",
    "states, preferred_actions, rejected_actions = generate_preference_dataset(pi1, pi2, dataset_size=dataset_size, env=env, max_t=max_t, seed=seed, dim_state=4, print_every=print_every, sampling_method=sampling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5230e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)\n",
    "print(max(states[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf06cf",
   "metadata": {},
   "source": [
    "### Uniform / normal sampling\n",
    "\n",
    "The coordinates of the states $s_i$ are either sampled from a uniform distribution (for the positions) or from a standard normal distribution (for the velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "189ac734",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "seed=0\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "max_t = 999\n",
    "sampling_method='uniform_cartpole'\n",
    "\n",
    "pi1_path = os.path.join(PATH, 'saved_policies', 'pi1_cartpole_0.pt')\n",
    "pi1 = Policy().to(device)\n",
    "pi1.load_state_dict(torch.load(pi1_path, weights_only=True))\n",
    "\n",
    "pi2_path = os.path.join(PATH, 'saved_policies', 'pi2_cartpole_0.pt')\n",
    "pi2 = Policy().to(device)\n",
    "pi2.load_state_dict(torch.load(pi2_path, weights_only=True))\n",
    "\n",
    "states_uniform, preferred_actions_uniform, rejected_actions_uniform = generate_preference_dataset(pi1, pi2, dataset_size=dataset_size, env=env, max_t=max_t, seed=seed, dim_state=4, print_every=print_every, sampling_method=sampling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcff726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(states_uniform.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
